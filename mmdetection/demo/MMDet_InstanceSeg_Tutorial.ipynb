{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGYwt_UjIrqp"
   },
   "source": [
    "# Instance Segmentation\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- the basic structure of Mask R-CNN.\n",
    "- to perform inference with a MMDetection detector.\n",
    "- to train a new instance segmentation model with a new dataset.\n",
    "\n",
    "Let's start!\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/11MqMCyF_V7Rkw6b9CCShGfNTP59OJ905?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCk6uTQrdUUn"
   },
   "source": [
    "If you are running the tutorial files on the colab platform or a new virtual environment, please run the following code first to configure the runtime environment.\n",
    "```python\n",
    "!pip install -U openmim\n",
    "!mim install \"mmengine>=0.7.0\"\n",
    "!mim install \"mmcv>=2.0.0rc4\"\n",
    "\n",
    "# Install mmdetection\n",
    "!rm -rf mmdetection\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "\n",
    "!pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hD0mmMixT0p",
    "outputId": "221dad3c-5ef8-4094-e07e-289f333f7bb9"
   },
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(\"torch version:\",torch.__version__, \"cuda:\",torch.cuda.is_available())\n",
    "\n",
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(\"mmdetection:\",mmdet.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "import mmcv\n",
    "print(\"mmcv:\",mmcv.__version__)\n",
    "\n",
    "# Check mmengine installation\n",
    "import mmengine\n",
    "print(\"mmengine:\",mmengine.__version__)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi9zw03oM4CH"
   },
   "source": [
    "## Perform Inference with An MMDetection Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pFYLerc0we1"
   },
   "source": [
    "### A two-stage detector\n",
    "\n",
    "In this tutorial, we use Mask R-CNN, a simple two-stage detector as an example.\n",
    "\n",
    "The high-level architecture of Mask R-CNN is shown in the following picture. More details can be found in the [paper](https://arxiv.org/abs/1703.06870).\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/40661020/143967081-c2552bed-9af2-46c4-ae44-5b3b74e5679f.png\" alt=\"mask rcnn\" align=\"bottom\" />\n",
    "\n",
    "Mask R-CNN adds a mask branch based on the original Faster R-CNN. It also uses RoIAlign, a more precise version of RoIPooling for RoI feature extraction to improve the performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWI-nX5yRYYQ",
    "outputId": "fd91e337-27cb-492c-a948-98adcbcfca27"
   },
   "source": [
    "!mim download mmdet --config mask-rcnn_r50-caffe_fpn_ms-poly-3x_coco --dest ./checkpoints"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8M5KUnX7Np3h",
    "outputId": "71de79c0-9f7e-4cae-f810-5c0a20fe9be8"
   },
   "source": [
    "import mmcv\n",
    "import mmengine\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "# Choose to use a config and initialize the detector\n",
    "config_file = 'configs/mask_rcnn/mask-rcnn_r50-caffe_fpn_ms-poly-3x_coco.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint_file = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "\n",
    "# register all modules in mmdet into the registries\n",
    "register_all_modules()\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')  # or device='cuda:0'\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVqDQAOiKkJK"
   },
   "source": [
    "From the printed model, we will find that the model does consist of the components that we described earlier. It uses ResNet as its CNN backbone, and has a RPN head and RoI Head.\n",
    "The RoI Head includes box head and mask head. In addition, the model has a neural network module, named neck, directly after the CNN backbone. It is a [feature pyramid network (FPN)](https://arxiv.org/abs/1612.03144) for enhancing the multi-scale features.\n",
    "\n",
    "\n",
    "### Inference with the detector\n",
    "\n",
    "The model is successfully created and loaded, let's see how good it is. We use the high-level API `inference_detector` implemented in the MMDetection. This API is created to ease the inference process. The details of the codes can be found [here](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/apis/inference.py#L15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi6DRpsQPEmV",
    "outputId": "42a9dd39-edcb-49f1-e318-a3cd77f89eee"
   },
   "source": [
    "# Use the detector to do inference\n",
    "image = mmcv.imread('demo/demo.jpg',channel_order='rgb')\n",
    "result = inference_detector(model, image)\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pFVhKeQRYYS"
   },
   "source": [
    "### Let's plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YinmJV1dRYYT",
    "outputId": "e6c9059f-55b3-481b-edef-b21befcbcf2e"
   },
   "source": [
    "from mmdet.registry import VISUALIZERS\n",
    "# init visualizer(run the block only once in jupyter notebook)\n",
    "visualizer = VISUALIZERS.build(model.cfg.visualizer)\n",
    "# the dataset_meta is loaded from the checkpoint and\n",
    "# then pass to the model in init_detector\n",
    "visualizer.dataset_meta = model.dataset_meta"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "z6qT6pG1RYYT",
    "outputId": "089b652b-061f-480d-f9de-ffa06b7d385a"
   },
   "source": [
    "# show the results\n",
    "visualizer.add_datasample(\n",
    "    'result',\n",
    "    image,\n",
    "    data_sample=result,\n",
    "    draw_gt = None,\n",
    "    wait_time=0,\n",
    ")\n",
    "visualizer.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GrWIJywLV-V"
   },
   "source": [
    "## Train a Detector on A Customized Dataset\n",
    "\n",
    "To train a new detector, there are usually three things to do:\n",
    "1. Support a new dataset\n",
    "2. Modify the config\n",
    "3. Train a new detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E73y5Lru-wBx"
   },
   "source": [
    "### Support a new dataset\n",
    "\n",
    "There are three ways to support a new dataset in MMDetection:\n",
    "  1. Reorganize the dataset into a COCO format\n",
    "  2. Reorganize the dataset into a middle format\n",
    "  3. Implement a new dataset\n",
    "\n",
    "We recommend the first two methods, as they are usually easier than the third.\n",
    "\n",
    "In this tutorial, we give an example that converts the data into COCO format because MMDetection **only support evaluating mask AP of dataset in COCO format for now**. Other methods and more advanced usages can be found in the [doc](https://mmdetection.readthedocs.io/en/latest/advanced_guides/customize_dataset.html).\n",
    "\n",
    "First, let's download the [the balloon dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHnw5Q_nARXq"
   },
   "source": [
    "# download and unzip the data\n",
    "!wget -c https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucSfn1U_RYYW"
   },
   "source": [
    "!unzip balloon_dataset.zip -d ./ballondatasets/"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro6JhfBVRYYX"
   },
   "source": [
    "# Check the directory structure of the tiny data\n",
    "\n",
    "# Install tree first in your terminal(linux)\n",
    "sudo apt-get -q install tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wuwxw1oZRtVZ",
    "outputId": "2e472cfa-2e2f-41ea-ddec-5a9d49fe71cf"
   },
   "source": [
    "!tree ballondatasets"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "id": "YnQQqzOWzE91",
    "outputId": "ff7d3804-638c-461f-aef6-8c496a4b69c8"
   },
   "source": [
    "# Let's take a look at the dataset image\n",
    "import mmcv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = mmcv.imread('ballondatasets/balloon/train/10464445726_6f1e3bbe6a_k.jpg')\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(mmcv.bgr2rgb(img))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMZvtSIl71qi"
   },
   "source": [
    "After downloading the data, we need to implement a function to convert the annotation format into the COCO format. Then we can use implemented `COCODataset` to load the data and perform training and evaluation.\n",
    "Let's take a look at the annotation json file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "n7rwalnPd6e1"
   },
   "source": [
    "# Check the label of a single image\n",
    "import mmengine\n",
    "\n",
    "annotation = mmengine.load('./ballondatasets/balloon/train/via_region_data.json')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keLW7uqJM54Y",
    "outputId": "8bdf087e-5ec0-4f8a-ee1d-5692986ac87d"
   },
   "source": [
    "# The annotation is a dict, and its values looks like the following\n",
    "annotation['34020010494_e5cb88e1c4_k.jpg1115004']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA1pFg-FeO3l"
   },
   "source": [
    "According to the above observation, each single image has a corresponding annotation containing keys `filename` and `regions` that are necessary for training.\n",
    "We need to read annotations of each image and convert them into COCO format as below:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"images\": [image],\n",
    "    \"annotations\": [annotation],\n",
    "    \"categories\": [category]\n",
    "}\n",
    "\n",
    "\n",
    "image = {\n",
    "    \"id\": int,\n",
    "    \"width\": int,\n",
    "    \"height\": int,\n",
    "    \"file_name\": str,\n",
    "}\n",
    "\n",
    "annotation = {\n",
    "    \"id\": int,\n",
    "    \"image_id\": int,\n",
    "    \"category_id\": int,\n",
    "    \"segmentation\": RLE or [polygon],\n",
    "    \"area\": float,\n",
    "    \"bbox\": [x,y,width,height],\n",
    "    \"iscrowd\": 0 or 1,\n",
    "}\n",
    "\n",
    "categories = [{\n",
    "    \"id\": int,\n",
    "    \"name\": str,\n",
    "    \"supercategory\": str,\n",
    "}]\n",
    "```\n",
    "**Note**: We only list the necessary keys for training, as shown above. For a full COCO format, please see [here](https://cocodataset.org/#format-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GdSaB2ad0EdX"
   },
   "source": [
    "import os.path as osp\n",
    "\n",
    "def convert_balloon_to_coco(ann_file, out_file, image_prefix):\n",
    "    data_infos = mmengine.load(ann_file)\n",
    "\n",
    "    annotations = []\n",
    "    images = []\n",
    "    obj_count = 0\n",
    "    for idx, v in enumerate(mmengine.track_iter_progress(list(data_infos.values()))):\n",
    "        filename = v['filename']\n",
    "        img_path = osp.join(image_prefix, filename)\n",
    "        height, width = mmcv.imread(img_path).shape[:2]\n",
    "\n",
    "        images.append(dict(\n",
    "            id=idx,\n",
    "            file_name=filename,\n",
    "            height=height,\n",
    "            width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        for _, obj in v['regions'].items():\n",
    "            assert not obj['region_attributes']\n",
    "            obj = obj['shape_attributes']\n",
    "            px = obj['all_points_x']\n",
    "            py = obj['all_points_y']\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            x_min, y_min, x_max, y_max = (\n",
    "                min(px), min(py), max(px), max(py))\n",
    "\n",
    "\n",
    "            data_anno = dict(\n",
    "                image_id=idx,\n",
    "                id=obj_count,\n",
    "                category_id=0,\n",
    "                bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                area=(x_max - x_min) * (y_max - y_min),\n",
    "                segmentation=[poly],\n",
    "                iscrowd=0)\n",
    "            annotations.append(data_anno)\n",
    "            obj_count += 1\n",
    "\n",
    "    coco_format_json = dict(\n",
    "        images=images,\n",
    "        annotations=annotations,\n",
    "        categories=[{'id':0, 'name': 'balloon'}])\n",
    "    mmengine.dump(coco_format_json, out_file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3xV5ktqlpFu",
    "outputId": "2d97137b-34e6-42e5-c8d6-0a4fe7d2c7cf"
   },
   "source": [
    "convert_balloon_to_coco(\n",
    "    './ballondatasets/balloon/train/via_region_data.json',\n",
    "    './ballondatasets/balloon/train/annotation_coco.json',\n",
    "    './ballondatasets/balloon/train/')\n",
    "convert_balloon_to_coco(\n",
    "    './ballondatasets/balloon/val/via_region_data.json',\n",
    "    './ballondatasets/balloon/val/annotation_coco.json',\n",
    "    './ballondatasets/balloon/val/')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h85AtunjRvx4"
   },
   "source": [
    "Checking the label corresponding to the instance split ID after the data format conversion is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaYkWbxORwZq",
    "outputId": "02ad1ff6-f138-49af-b733-1d23c51557f5"
   },
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Path to load the COCO annotation file\n",
    "annotation_file = './ballondatasets/balloon/train/annotation_coco.json'\n",
    "\n",
    "# Initialise the COCO object\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Get all category tags and corresponding category IDs\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "category_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "\n",
    "# Print all category IDs and corresponding category names\n",
    "for category_id, category_name in category_id_to_name.items():\n",
    "    print(f\"Category ID: {category_id}, Category Name: {category_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwqJOpBe-bMj"
   },
   "source": [
    "### Modify the config\n",
    "\n",
    "In the next step, we need to modify the config for the training.\n",
    "To accelerate the process, we finetune a detector using a pre-trained detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hamZrlnH-YDD"
   },
   "source": [
    "from mmengine import Config\n",
    "cfg = Config.fromfile('./configs/mask_rcnn/mask-rcnn_r50-caffe_fpn_ms-poly-1x_coco.py')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HntziLGq-92Z"
   },
   "source": [
    "Given a config that trains a Mask R-CNN on COCO dataset, we need to modify some values to use it for training on the balloon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pUbwD8uV0PR8"
   },
   "source": [
    "from mmengine.runner import set_random_seed\n",
    "\n",
    "# Modify dataset classes and color\n",
    "cfg.metainfo = {\n",
    "    'classes': ('balloon', ),\n",
    "    'palette': [\n",
    "        (220, 20, 60),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.data_root = './ballondatasets/balloon'\n",
    "\n",
    "cfg.train_dataloader.dataset.ann_file = 'train/annotation_coco.json'\n",
    "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.train_dataloader.dataset.data_prefix.img = 'train/'\n",
    "cfg.train_dataloader.dataset.metainfo = cfg.metainfo\n",
    "\n",
    "cfg.val_dataloader.dataset.ann_file = 'val/annotation_coco.json'\n",
    "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.val_dataloader.dataset.data_prefix.img = 'val/'\n",
    "cfg.val_dataloader.dataset.metainfo = cfg.metainfo\n",
    "\n",
    "cfg.test_dataloader = cfg.val_dataloader\n",
    "\n",
    "# Modify metric config\n",
    "cfg.val_evaluator.ann_file = cfg.data_root+'/'+'val/annotation_coco.json'\n",
    "cfg.test_evaluator = cfg.val_evaluator\n",
    "\n",
    "# Modify num classes of the model in box head and mask head\n",
    "cfg.model.roi_head.bbox_head.num_classes = 1\n",
    "cfg.model.roi_head.mask_head.num_classes = 1\n",
    "\n",
    "# We can still the pre-trained Mask RCNN model to obtain a higher performance\n",
    "cfg.load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './tutorial_exps'\n",
    "\n",
    "\n",
    "# We can set the evaluation interval to reduce the evaluation times\n",
    "cfg.train_cfg.val_interval = 3\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.default_hooks.checkpoint.interval = 3\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.optim_wrapper.optimizer.lr = 0.02 / 8\n",
    "cfg.default_hooks.logger.interval = 10\n",
    "\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "# cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "\n",
    "# We can also use tensorboard to log the training process\n",
    "cfg.visualizer.vis_backends.append({\"type\":'TensorboardVisBackend'})\n",
    "\n",
    "#------------------------------------------------------\n",
    "config=f'./configs/mask_rcnn/mask-rcnn_r50-caffe_fpn_ms-poly-3x_balloon.py'\n",
    "with open(config, 'w') as f:\n",
    "    f.write(cfg.pretty_text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "111W_oZV_3wa"
   },
   "source": [
    "### Train a new detector\n",
    "\n",
    "Finally, lets initialize the dataset and detector, then train a new detector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiqDnPdAMGyg",
    "outputId": "0de25679-3541-488e-eceb-5b5400f92745"
   },
   "source": [
    "!python tools/train.py {config}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vYQF5K2NqqI"
   },
   "source": [
    "### Understand the log\n",
    "From the log, we can have a basic understanding on the training process and know how well the detector is trained.\n",
    "\n",
    "First, since the dataset we are using is small, we loaded a Mask R-CNN model and finetune it for detection. Because the original Mask R-CNN is trained on COCO dataset that contains 80 classes but KITTI Tiny dataset only have 3 classes. Therefore, the last FC layers of the pre-trained Mask R-CNN for classification and regression have different weight shape and are not used. The pre-trained weights of mask prediction layer `mask_head.conv_logits` also does not matches the current model and is not used due to similar reason.\n",
    "\n",
    "Third, after training, the detector is evaluated by the default COCO-style evaluation. The results show that the detector achieves 79.6 bbox AP and 81.5 mask AP on the val dataset, not bad!\n",
    "\n",
    " We can also check the tensorboard to see the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbLNlJR-RYYd"
   },
   "source": [
    "%pip install tensorboard  -i https://mirrors.ustc.edu.cn/pypi/web/simple"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PW2NAam_7irv"
   },
   "source": [
    "# load tensorboard in jupyter notebook\n",
    "%load_ext tensorboard"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4G9MCbL2RYYd"
   },
   "source": [
    "# see curves in tensorboard\n",
    "# if you see <IPython.core.display.HTML object> please run it again\n",
    "%tensorboard --logdir tutorial_exps/"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfQ-yspZLuuI"
   },
   "source": [
    "## Test the Trained Detector\n",
    "\n",
    "After finetuning the detector, let's visualize the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MuZurfGLq0p",
    "outputId": "4b25759c-8e22-405e-a061-3abc44e38043"
   },
   "source": [
    "import mmcv\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "img = mmcv.imread('./ballondatasets/balloon/train/7178882742_f090f3ce56_k.jpg',channel_order='rgb')\n",
    "checkpoint_file = 'tutorial_exps/epoch_12.pth'\n",
    "model = init_detector(cfg, checkpoint_file, device='cpu')\n",
    "new_result = inference_detector(model, img)\n",
    "print(new_result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "7SSTauCURYYe",
    "outputId": "3becb5ea-cb4e-44f6-d93d-c10194a2263b"
   },
   "source": [
    "from mmengine.visualization import Visualizer\n",
    "# get built visualizer\n",
    "visualizer_now = Visualizer.get_current_instance()\n",
    "# the dataset_meta is loaded from the checkpoint and\n",
    "# then pass to the model in init_detector\n",
    "visualizer_now.dataset_meta = model.dataset_meta\n",
    "# show the results\n",
    "visualizer_now.add_datasample(\n",
    "    'new_result',\n",
    "    img,\n",
    "    data_sample=new_result,\n",
    "    draw_gt=False,\n",
    "    wait_time=0,\n",
    "    out_file=None,\n",
    "    pred_score_thr=0.5\n",
    ")\n",
    "visualizer_now.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rzruCwFgPXm"
   },
   "source": [
    "## What to Do Next?\n",
    "\n",
    "So far, we have learnt how to test and train Mask R-CNN. To further explore the segmentation task, you could do several other things as shown below:\n",
    "\n",
    "- Try cascade methods, e.g., [Cascade Mask R-CNN](https://github.com/open-mmlab/mmdetection/tree/master/configs/cascade_rcnn) and [HTC](https://github.com/open-mmlab/mmdetection/tree/master/configs/htc) in [MMDetection model zoo](https://github.com/open-mmlab/mmdetection/blob/master/docs/en/model_zoo.md). They are powerful detectors that are ranked high in many benchmarks, e.g., COCO dataset.\n",
    "- Try single-stage methods, e.g., [K-Net](https://github.com/ZwwWayne/K-Net) and [Dense-RepPoints](https://github.com/justimyhxu/Dense-RepPoints). These two algorithms are based on MMDetection. Box-free instance segmentation is a new trend in the instance segmentation community.\n",
    "- Try semantic segmentation. Semantic segmentation is also a popular task with wide applications. You can explore [MMSegmentation](https://github.com/open-mmlab/mmsegmentation/); we also provide a [colab tutorial](https://github.com/open-mmlab/mmsegmentation/blob/master/demo/MMSegmentation_Tutorial.ipynb) for semantic segmentation using MMSegmentation.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8868640c17582ff5a3e06365ba2fb344ce697cf42d4745ae8b85a9738303c037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
